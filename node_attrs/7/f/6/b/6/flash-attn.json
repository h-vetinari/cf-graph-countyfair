{
 "archived": false,
 "branch": "main",
 "conda-forge.yml": {
  "build_platform": {
   "linux_aarch64": "linux_64"
  },
  "conda_build": {
   "error_overlinking": true
  },
  "conda_forge_output_validation": true,
  "github": {
   "branch_name": "main",
   "tooling_branch_name": "main"
  },
  "github_actions": {
   "self_hosted": true,
   "timeout_minutes": 1080,
   "triggers": [
    "push",
    "pull_request"
   ]
  },
  "provider": {
   "linux_64": "github_actions",
   "linux_aarch64": "github_actions"
  }
 },
 "feedstock_name": "flash-attn",
 "hash_type": "sha256",
 "linux_64_meta_yaml": {
  "about": {
   "home": "https://github.com/Dao-AILab/flash-attention",
   "license": "BSD-3-Clause",
   "license_file": [
    "LICENSE",
    "LICENSE_CUTLASS.txt"
   ],
   "summary": "Flash Attention: Fast and Memory-Efficient Exact Attention"
  },
  "build": {
   "number": "0",
   "script": "PYTHON -m pip install . -vvv --no-deps --no-build-isolation",
   "script_env": [
    "MAX_JOBS=4",
    "TORCH_CUDA_ARCH_LIST=8.0;8.6;8.9;9.0+PTX"
   ]
  },
  "extra": {
   "feedstock-name": "flash-attn",
   "recipe-maintainers": [
    "carterbox",
    "weiji14"
   ]
  },
  "outputs": [
   {
    "files": {
     "include": [
      "lib/python*/site-packages/flash_attn/**",
      "lib/python*/site-packages/flash_attn-2.7.4.post1.dist-info/**",
      "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
     ]
    },
    "name": "flash-attn",
    "requirements": {
     "build": [
      "c_compiler_stub",
      "cxx_compiler_stub",
      "cuda_compiler_stub",
      "c_stdlib_stub"
     ],
     "host": [
      "cuda-version 12.6",
      "cuda-cudart-dev",
      "python",
      "libtorch",
      "pytorch",
      "pytorch =*=cuda*"
     ],
     "run": [
      "einops",
      "python",
      "pytorch =*=cuda*"
     ]
    },
    "test": {
     "commands": [
      "pip check"
     ],
     "imports": [
      "flash_attn"
     ],
     "requires": [
      "pip"
     ]
    }
   },
   {
    "files": {
     "include": [
      "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
     ]
    },
    "name": "flash-attn-fused-dense",
    "requirements": {
     "build": [
      "cxx_compiler_stub",
      "c_stdlib_stub",
      "cuda_compiler_stub"
     ],
     "host": [
      "cuda-version 12.6",
      "cuda-cudart-dev",
      "libcublas-dev",
      "python",
      "pytorch",
      "pytorch =*=cuda*"
     ],
     "run": [
      "python",
      "flash-attn"
     ]
    },
    "test": {
     "commands": [
      "pip check"
     ],
     "imports": [
      "flash_attn.ops.fused_dense"
     ],
     "requires": [
      "pip"
     ]
    }
   },
   {
    "files": {
     "include": [
      "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
     ]
    },
    "name": "flash-attn-layer-norm",
    "requirements": {
     "build": [
      "cxx_compiler_stub",
      "c_stdlib_stub",
      "cuda_compiler_stub"
     ],
     "host": [
      "cuda-version 12.6",
      "cuda-cudart-dev",
      "python",
      "pytorch",
      "pytorch =*=cuda*"
     ],
     "run": [
      "python",
      "flash-attn"
     ]
    },
    "test": {
     "commands": [
      "pip check"
     ],
     "imports": [
      "flash_attn.ops.layer_norm"
     ],
     "requires": [
      "pip"
     ]
    }
   },
   {
    "files": {
     "include": [
      "lib/python*/site-packages/flash_attn/**",
      "lib/python*/site-packages/flash_attn-2.7.4.post1.dist-info/**",
      "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
     ]
    },
    "name": "flash-attn",
    "requirements": {
     "build": [
      "c_compiler_stub",
      "cxx_compiler_stub",
      "cuda_compiler_stub",
      "c_stdlib_stub"
     ],
     "host": [
      "cuda-version 12.6",
      "cuda-cudart-dev",
      "python",
      "libtorch",
      "pytorch",
      "pytorch =*=cuda*"
     ],
     "run": [
      "einops",
      "python",
      "pytorch =*=cuda*"
     ]
    },
    "test": {
     "commands": [
      "pip check"
     ],
     "imports": [
      "flash_attn"
     ],
     "requires": [
      "pip"
     ]
    }
   },
   {
    "files": {
     "include": [
      "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
     ]
    },
    "name": "flash-attn-fused-dense",
    "requirements": {
     "build": [
      "cxx_compiler_stub",
      "c_stdlib_stub",
      "cuda_compiler_stub"
     ],
     "host": [
      "cuda-version 12.6",
      "cuda-cudart-dev",
      "libcublas-dev",
      "python",
      "pytorch",
      "pytorch =*=cuda*"
     ],
     "run": [
      "python",
      "flash-attn"
     ]
    },
    "test": {
     "commands": [
      "pip check"
     ],
     "imports": [
      "flash_attn.ops.fused_dense"
     ],
     "requires": [
      "pip"
     ]
    }
   },
   {
    "files": {
     "include": [
      "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
     ]
    },
    "name": "flash-attn-layer-norm",
    "requirements": {
     "build": [
      "cxx_compiler_stub",
      "c_stdlib_stub",
      "cuda_compiler_stub"
     ],
     "host": [
      "cuda-version 12.6",
      "cuda-cudart-dev",
      "python",
      "pytorch",
      "pytorch =*=cuda*"
     ],
     "run": [
      "python",
      "flash-attn"
     ]
    },
    "test": {
     "commands": [
      "pip check"
     ],
     "imports": [
      "flash_attn.ops.layer_norm"
     ],
     "requires": [
      "pip"
     ]
    }
   },
   {
    "files": {
     "include": [
      "lib/python*/site-packages/flash_attn/**",
      "lib/python*/site-packages/flash_attn-2.7.4.post1.dist-info/**",
      "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
     ]
    },
    "name": "flash-attn",
    "requirements": {
     "build": [
      "c_compiler_stub",
      "cxx_compiler_stub",
      "cuda_compiler_stub",
      "c_stdlib_stub"
     ],
     "host": [
      "cuda-version 12.6",
      "cuda-cudart-dev",
      "python",
      "libtorch",
      "pytorch",
      "pytorch =*=cuda*"
     ],
     "run": [
      "einops",
      "python",
      "pytorch =*=cuda*"
     ]
    },
    "test": {
     "commands": [
      "pip check"
     ],
     "imports": [
      "flash_attn"
     ],
     "requires": [
      "pip"
     ]
    }
   },
   {
    "files": {
     "include": [
      "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
     ]
    },
    "name": "flash-attn-fused-dense",
    "requirements": {
     "build": [
      "cxx_compiler_stub",
      "c_stdlib_stub",
      "cuda_compiler_stub"
     ],
     "host": [
      "cuda-version 12.6",
      "cuda-cudart-dev",
      "libcublas-dev",
      "python",
      "pytorch",
      "pytorch =*=cuda*"
     ],
     "run": [
      "python",
      "flash-attn"
     ]
    },
    "test": {
     "commands": [
      "pip check"
     ],
     "imports": [
      "flash_attn.ops.fused_dense"
     ],
     "requires": [
      "pip"
     ]
    }
   },
   {
    "files": {
     "include": [
      "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
     ]
    },
    "name": "flash-attn-layer-norm",
    "requirements": {
     "build": [
      "cxx_compiler_stub",
      "c_stdlib_stub",
      "cuda_compiler_stub"
     ],
     "host": [
      "cuda-version 12.6",
      "cuda-cudart-dev",
      "python",
      "pytorch",
      "pytorch =*=cuda*"
     ],
     "run": [
      "python",
      "flash-attn"
     ]
    },
    "test": {
     "commands": [
      "pip check"
     ],
     "imports": [
      "flash_attn.ops.layer_norm"
     ],
     "requires": [
      "pip"
     ]
    }
   },
   {
    "files": {
     "include": [
      "lib/python*/site-packages/flash_attn/**",
      "lib/python*/site-packages/flash_attn-2.7.4.post1.dist-info/**",
      "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
     ]
    },
    "name": "flash-attn",
    "requirements": {
     "build": [
      "c_compiler_stub",
      "cxx_compiler_stub",
      "cuda_compiler_stub",
      "c_stdlib_stub"
     ],
     "host": [
      "cuda-version 12.6",
      "cuda-cudart-dev",
      "python",
      "libtorch",
      "pytorch",
      "pytorch =*=cuda*"
     ],
     "run": [
      "einops",
      "python",
      "pytorch =*=cuda*"
     ]
    },
    "test": {
     "commands": [
      "pip check"
     ],
     "imports": [
      "flash_attn"
     ],
     "requires": [
      "pip"
     ]
    }
   },
   {
    "files": {
     "include": [
      "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
     ]
    },
    "name": "flash-attn-fused-dense",
    "requirements": {
     "build": [
      "cxx_compiler_stub",
      "c_stdlib_stub",
      "cuda_compiler_stub"
     ],
     "host": [
      "cuda-version 12.6",
      "cuda-cudart-dev",
      "libcublas-dev",
      "python",
      "pytorch",
      "pytorch =*=cuda*"
     ],
     "run": [
      "python",
      "flash-attn"
     ]
    },
    "test": {
     "commands": [
      "pip check"
     ],
     "imports": [
      "flash_attn.ops.fused_dense"
     ],
     "requires": [
      "pip"
     ]
    }
   },
   {
    "files": {
     "include": [
      "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
     ]
    },
    "name": "flash-attn-layer-norm",
    "requirements": {
     "build": [
      "cxx_compiler_stub",
      "c_stdlib_stub",
      "cuda_compiler_stub"
     ],
     "host": [
      "cuda-version 12.6",
      "cuda-cudart-dev",
      "python",
      "pytorch",
      "pytorch =*=cuda*"
     ],
     "run": [
      "python",
      "flash-attn"
     ]
    },
    "test": {
     "commands": [
      "pip check"
     ],
     "imports": [
      "flash_attn.ops.layer_norm"
     ],
     "requires": [
      "pip"
     ]
    }
   },
   {
    "files": {
     "include": [
      "lib/python*/site-packages/flash_attn/**",
      "lib/python*/site-packages/flash_attn-2.7.4.post1.dist-info/**",
      "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
     ]
    },
    "name": "flash-attn",
    "requirements": {
     "build": [
      "c_compiler_stub",
      "cxx_compiler_stub",
      "cuda_compiler_stub",
      "c_stdlib_stub"
     ],
     "host": [
      "cuda-version 12.6",
      "cuda-cudart-dev",
      "python",
      "libtorch",
      "pytorch",
      "pytorch =*=cuda*"
     ],
     "run": [
      "einops",
      "python",
      "pytorch =*=cuda*"
     ]
    },
    "test": {
     "commands": [
      "pip check"
     ],
     "imports": [
      "flash_attn"
     ],
     "requires": [
      "pip"
     ]
    }
   },
   {
    "files": {
     "include": [
      "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
     ]
    },
    "name": "flash-attn-fused-dense",
    "requirements": {
     "build": [
      "cxx_compiler_stub",
      "c_stdlib_stub",
      "cuda_compiler_stub"
     ],
     "host": [
      "cuda-version 12.6",
      "cuda-cudart-dev",
      "libcublas-dev",
      "python",
      "pytorch",
      "pytorch =*=cuda*"
     ],
     "run": [
      "python",
      "flash-attn"
     ]
    },
    "test": {
     "commands": [
      "pip check"
     ],
     "imports": [
      "flash_attn.ops.fused_dense"
     ],
     "requires": [
      "pip"
     ]
    }
   },
   {
    "files": {
     "include": [
      "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
     ]
    },
    "name": "flash-attn-layer-norm",
    "requirements": {
     "build": [
      "cxx_compiler_stub",
      "c_stdlib_stub",
      "cuda_compiler_stub"
     ],
     "host": [
      "cuda-version 12.6",
      "cuda-cudart-dev",
      "python",
      "pytorch",
      "pytorch =*=cuda*"
     ],
     "run": [
      "python",
      "flash-attn"
     ]
    },
    "test": {
     "commands": [
      "pip check"
     ],
     "imports": [
      "flash_attn.ops.layer_norm"
     ],
     "requires": [
      "pip"
     ]
    }
   }
  ],
  "package": {
   "name": "flash-attn-split",
   "version": "2.7.4"
  },
  "requirements": {
   "build": [
    "c_compiler_stub",
    "cxx_compiler_stub",
    "cuda_compiler_stub",
    "c_stdlib_stub",
    "ninja"
   ],
   "host": [
    "cuda-version 12.6",
    "cuda-cudart-dev",
    "libcublas-dev",
    "libcurand-dev",
    "libcusolver-dev",
    "libcusparse-dev",
    "libtorch",
    "pip",
    "python",
    "pytorch",
    "pytorch =*=cuda*",
    "setuptools"
   ]
  },
  "schema_version": 0,
  "source": [
   {
    "sha256": "f03485c9a49a4d68d0733acdcad80ab0e72afa025a777fdc2966ceccf9d51765",
    "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.7.4.post1.tar.gz"
   },
   {
    "path": "pyproject.toml"
   },
   {
    "path": "setup.py"
   },
   {
    "sha256": "f03485c9a49a4d68d0733acdcad80ab0e72afa025a777fdc2966ceccf9d51765",
    "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.7.4.post1.tar.gz"
   },
   {
    "path": "pyproject.toml"
   },
   {
    "path": "setup.py"
   },
   {
    "sha256": "f03485c9a49a4d68d0733acdcad80ab0e72afa025a777fdc2966ceccf9d51765",
    "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.7.4.post1.tar.gz"
   },
   {
    "path": "pyproject.toml"
   },
   {
    "path": "setup.py"
   },
   {
    "sha256": "f03485c9a49a4d68d0733acdcad80ab0e72afa025a777fdc2966ceccf9d51765",
    "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.7.4.post1.tar.gz"
   },
   {
    "path": "pyproject.toml"
   },
   {
    "path": "setup.py"
   },
   {
    "sha256": "f03485c9a49a4d68d0733acdcad80ab0e72afa025a777fdc2966ceccf9d51765",
    "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.7.4.post1.tar.gz"
   },
   {
    "path": "pyproject.toml"
   },
   {
    "path": "setup.py"
   }
  ]
 },
 "linux_64_requirements": {
  "build": {
   "__set__": true,
   "elements": [
    "c_compiler_stub",
    "c_stdlib_stub",
    "cuda_compiler_stub",
    "cxx_compiler_stub",
    "ninja"
   ]
  },
  "host": {
   "__set__": true,
   "elements": [
    "cuda-cudart-dev",
    "cuda-version",
    "libcublas-dev",
    "libcurand-dev",
    "libcusolver-dev",
    "libcusparse-dev",
    "libtorch",
    "pip",
    "python",
    "pytorch",
    "setuptools"
   ]
  },
  "run": {
   "__set__": true,
   "elements": [
    "einops",
    "flash-attn",
    "python",
    "pytorch"
   ]
  },
  "test": {
   "__set__": true,
   "elements": [
    "pip"
   ]
  }
 },
 "linux_aarch64_meta_yaml": {
  "about": {
   "home": "https://github.com/Dao-AILab/flash-attention",
   "license": "BSD-3-Clause",
   "license_file": [
    "LICENSE",
    "LICENSE_CUTLASS.txt"
   ],
   "summary": "Flash Attention: Fast and Memory-Efficient Exact Attention"
  },
  "build": {
   "number": "0",
   "script": "PYTHON -m pip install . -vvv --no-deps --no-build-isolation",
   "script_env": [
    "MAX_JOBS=4",
    "TORCH_CUDA_ARCH_LIST=8.0;8.6;8.9;9.0+PTX"
   ]
  },
  "extra": {
   "feedstock-name": "flash-attn",
   "recipe-maintainers": [
    "carterbox",
    "weiji14"
   ]
  },
  "outputs": [
   {
    "files": {
     "include": [
      "lib/python*/site-packages/flash_attn/**",
      "lib/python*/site-packages/flash_attn-2.7.4.post1.dist-info/**",
      "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
     ]
    },
    "name": "flash-attn",
    "requirements": {
     "build": [
      "c_compiler_stub",
      "cxx_compiler_stub",
      "cuda_compiler_stub",
      "c_stdlib_stub"
     ],
     "host": [
      "cuda-version 12.6",
      "cuda-cudart-dev",
      "python",
      "libtorch",
      "pytorch",
      "pytorch =*=cuda*"
     ],
     "run": [
      "einops",
      "python",
      "pytorch =*=cuda*"
     ]
    },
    "test": {
     "commands": [
      "pip check"
     ],
     "imports": [
      "flash_attn"
     ],
     "requires": [
      "pip"
     ]
    }
   },
   {
    "files": {
     "include": [
      "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
     ]
    },
    "name": "flash-attn-fused-dense",
    "requirements": {
     "build": [
      "cxx_compiler_stub",
      "c_stdlib_stub",
      "cuda_compiler_stub"
     ],
     "host": [
      "cuda-version 12.6",
      "cuda-cudart-dev",
      "libcublas-dev",
      "python",
      "pytorch",
      "pytorch =*=cuda*"
     ],
     "run": [
      "python",
      "flash-attn"
     ]
    },
    "test": {
     "commands": [
      "pip check"
     ],
     "imports": [
      "flash_attn.ops.fused_dense"
     ],
     "requires": [
      "pip"
     ]
    }
   },
   {
    "files": {
     "include": [
      "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
     ]
    },
    "name": "flash-attn-layer-norm",
    "requirements": {
     "build": [
      "cxx_compiler_stub",
      "c_stdlib_stub",
      "cuda_compiler_stub"
     ],
     "host": [
      "cuda-version 12.6",
      "cuda-cudart-dev",
      "python",
      "pytorch",
      "pytorch =*=cuda*"
     ],
     "run": [
      "python",
      "flash-attn"
     ]
    },
    "test": {
     "commands": [
      "pip check"
     ],
     "imports": [
      "flash_attn.ops.layer_norm"
     ],
     "requires": [
      "pip"
     ]
    }
   },
   {
    "files": {
     "include": [
      "lib/python*/site-packages/flash_attn/**",
      "lib/python*/site-packages/flash_attn-2.7.4.post1.dist-info/**",
      "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
     ]
    },
    "name": "flash-attn",
    "requirements": {
     "build": [
      "c_compiler_stub",
      "cxx_compiler_stub",
      "cuda_compiler_stub",
      "c_stdlib_stub"
     ],
     "host": [
      "cuda-version 12.6",
      "cuda-cudart-dev",
      "python",
      "libtorch",
      "pytorch",
      "pytorch =*=cuda*"
     ],
     "run": [
      "einops",
      "python",
      "pytorch =*=cuda*"
     ]
    },
    "test": {
     "commands": [
      "pip check"
     ],
     "imports": [
      "flash_attn"
     ],
     "requires": [
      "pip"
     ]
    }
   },
   {
    "files": {
     "include": [
      "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
     ]
    },
    "name": "flash-attn-fused-dense",
    "requirements": {
     "build": [
      "cxx_compiler_stub",
      "c_stdlib_stub",
      "cuda_compiler_stub"
     ],
     "host": [
      "cuda-version 12.6",
      "cuda-cudart-dev",
      "libcublas-dev",
      "python",
      "pytorch",
      "pytorch =*=cuda*"
     ],
     "run": [
      "python",
      "flash-attn"
     ]
    },
    "test": {
     "commands": [
      "pip check"
     ],
     "imports": [
      "flash_attn.ops.fused_dense"
     ],
     "requires": [
      "pip"
     ]
    }
   },
   {
    "files": {
     "include": [
      "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
     ]
    },
    "name": "flash-attn-layer-norm",
    "requirements": {
     "build": [
      "cxx_compiler_stub",
      "c_stdlib_stub",
      "cuda_compiler_stub"
     ],
     "host": [
      "cuda-version 12.6",
      "cuda-cudart-dev",
      "python",
      "pytorch",
      "pytorch =*=cuda*"
     ],
     "run": [
      "python",
      "flash-attn"
     ]
    },
    "test": {
     "commands": [
      "pip check"
     ],
     "imports": [
      "flash_attn.ops.layer_norm"
     ],
     "requires": [
      "pip"
     ]
    }
   },
   {
    "files": {
     "include": [
      "lib/python*/site-packages/flash_attn/**",
      "lib/python*/site-packages/flash_attn-2.7.4.post1.dist-info/**",
      "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
     ]
    },
    "name": "flash-attn",
    "requirements": {
     "build": [
      "c_compiler_stub",
      "cxx_compiler_stub",
      "cuda_compiler_stub",
      "c_stdlib_stub"
     ],
     "host": [
      "cuda-version 12.6",
      "cuda-cudart-dev",
      "python",
      "libtorch",
      "pytorch",
      "pytorch =*=cuda*"
     ],
     "run": [
      "einops",
      "python",
      "pytorch =*=cuda*"
     ]
    },
    "test": {
     "commands": [
      "pip check"
     ],
     "imports": [
      "flash_attn"
     ],
     "requires": [
      "pip"
     ]
    }
   },
   {
    "files": {
     "include": [
      "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
     ]
    },
    "name": "flash-attn-fused-dense",
    "requirements": {
     "build": [
      "cxx_compiler_stub",
      "c_stdlib_stub",
      "cuda_compiler_stub"
     ],
     "host": [
      "cuda-version 12.6",
      "cuda-cudart-dev",
      "libcublas-dev",
      "python",
      "pytorch",
      "pytorch =*=cuda*"
     ],
     "run": [
      "python",
      "flash-attn"
     ]
    },
    "test": {
     "commands": [
      "pip check"
     ],
     "imports": [
      "flash_attn.ops.fused_dense"
     ],
     "requires": [
      "pip"
     ]
    }
   },
   {
    "files": {
     "include": [
      "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
     ]
    },
    "name": "flash-attn-layer-norm",
    "requirements": {
     "build": [
      "cxx_compiler_stub",
      "c_stdlib_stub",
      "cuda_compiler_stub"
     ],
     "host": [
      "cuda-version 12.6",
      "cuda-cudart-dev",
      "python",
      "pytorch",
      "pytorch =*=cuda*"
     ],
     "run": [
      "python",
      "flash-attn"
     ]
    },
    "test": {
     "commands": [
      "pip check"
     ],
     "imports": [
      "flash_attn.ops.layer_norm"
     ],
     "requires": [
      "pip"
     ]
    }
   },
   {
    "files": {
     "include": [
      "lib/python*/site-packages/flash_attn/**",
      "lib/python*/site-packages/flash_attn-2.7.4.post1.dist-info/**",
      "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
     ]
    },
    "name": "flash-attn",
    "requirements": {
     "build": [
      "c_compiler_stub",
      "cxx_compiler_stub",
      "cuda_compiler_stub",
      "c_stdlib_stub"
     ],
     "host": [
      "cuda-version 12.6",
      "cuda-cudart-dev",
      "python",
      "libtorch",
      "pytorch",
      "pytorch =*=cuda*"
     ],
     "run": [
      "einops",
      "python",
      "pytorch =*=cuda*"
     ]
    },
    "test": {
     "commands": [
      "pip check"
     ],
     "imports": [
      "flash_attn"
     ],
     "requires": [
      "pip"
     ]
    }
   },
   {
    "files": {
     "include": [
      "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
     ]
    },
    "name": "flash-attn-fused-dense",
    "requirements": {
     "build": [
      "cxx_compiler_stub",
      "c_stdlib_stub",
      "cuda_compiler_stub"
     ],
     "host": [
      "cuda-version 12.6",
      "cuda-cudart-dev",
      "libcublas-dev",
      "python",
      "pytorch",
      "pytorch =*=cuda*"
     ],
     "run": [
      "python",
      "flash-attn"
     ]
    },
    "test": {
     "commands": [
      "pip check"
     ],
     "imports": [
      "flash_attn.ops.fused_dense"
     ],
     "requires": [
      "pip"
     ]
    }
   },
   {
    "files": {
     "include": [
      "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
     ]
    },
    "name": "flash-attn-layer-norm",
    "requirements": {
     "build": [
      "cxx_compiler_stub",
      "c_stdlib_stub",
      "cuda_compiler_stub"
     ],
     "host": [
      "cuda-version 12.6",
      "cuda-cudart-dev",
      "python",
      "pytorch",
      "pytorch =*=cuda*"
     ],
     "run": [
      "python",
      "flash-attn"
     ]
    },
    "test": {
     "commands": [
      "pip check"
     ],
     "imports": [
      "flash_attn.ops.layer_norm"
     ],
     "requires": [
      "pip"
     ]
    }
   },
   {
    "files": {
     "include": [
      "lib/python*/site-packages/flash_attn/**",
      "lib/python*/site-packages/flash_attn-2.7.4.post1.dist-info/**",
      "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
     ]
    },
    "name": "flash-attn",
    "requirements": {
     "build": [
      "c_compiler_stub",
      "cxx_compiler_stub",
      "cuda_compiler_stub",
      "c_stdlib_stub"
     ],
     "host": [
      "cuda-version 12.6",
      "cuda-cudart-dev",
      "python",
      "libtorch",
      "pytorch",
      "pytorch =*=cuda*"
     ],
     "run": [
      "einops",
      "python",
      "pytorch =*=cuda*"
     ]
    },
    "test": {
     "commands": [
      "pip check"
     ],
     "imports": [
      "flash_attn"
     ],
     "requires": [
      "pip"
     ]
    }
   },
   {
    "files": {
     "include": [
      "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
     ]
    },
    "name": "flash-attn-fused-dense",
    "requirements": {
     "build": [
      "cxx_compiler_stub",
      "c_stdlib_stub",
      "cuda_compiler_stub"
     ],
     "host": [
      "cuda-version 12.6",
      "cuda-cudart-dev",
      "libcublas-dev",
      "python",
      "pytorch",
      "pytorch =*=cuda*"
     ],
     "run": [
      "python",
      "flash-attn"
     ]
    },
    "test": {
     "commands": [
      "pip check"
     ],
     "imports": [
      "flash_attn.ops.fused_dense"
     ],
     "requires": [
      "pip"
     ]
    }
   },
   {
    "files": {
     "include": [
      "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
     ]
    },
    "name": "flash-attn-layer-norm",
    "requirements": {
     "build": [
      "cxx_compiler_stub",
      "c_stdlib_stub",
      "cuda_compiler_stub"
     ],
     "host": [
      "cuda-version 12.6",
      "cuda-cudart-dev",
      "python",
      "pytorch",
      "pytorch =*=cuda*"
     ],
     "run": [
      "python",
      "flash-attn"
     ]
    },
    "test": {
     "commands": [
      "pip check"
     ],
     "imports": [
      "flash_attn.ops.layer_norm"
     ],
     "requires": [
      "pip"
     ]
    }
   }
  ],
  "package": {
   "name": "flash-attn-split",
   "version": "2.7.4"
  },
  "requirements": {
   "build": [
    "c_compiler_stub",
    "cxx_compiler_stub",
    "cuda_compiler_stub",
    "c_stdlib_stub",
    "ninja"
   ],
   "host": [
    "cuda-version 12.6",
    "cuda-cudart-dev",
    "libcublas-dev",
    "libcurand-dev",
    "libcusolver-dev",
    "libcusparse-dev",
    "libtorch",
    "pip",
    "python",
    "pytorch",
    "pytorch =*=cuda*",
    "setuptools"
   ]
  },
  "schema_version": 0,
  "source": [
   {
    "sha256": "f03485c9a49a4d68d0733acdcad80ab0e72afa025a777fdc2966ceccf9d51765",
    "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.7.4.post1.tar.gz"
   },
   {
    "path": "pyproject.toml"
   },
   {
    "path": "setup.py"
   },
   {
    "sha256": "f03485c9a49a4d68d0733acdcad80ab0e72afa025a777fdc2966ceccf9d51765",
    "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.7.4.post1.tar.gz"
   },
   {
    "path": "pyproject.toml"
   },
   {
    "path": "setup.py"
   },
   {
    "sha256": "f03485c9a49a4d68d0733acdcad80ab0e72afa025a777fdc2966ceccf9d51765",
    "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.7.4.post1.tar.gz"
   },
   {
    "path": "pyproject.toml"
   },
   {
    "path": "setup.py"
   },
   {
    "sha256": "f03485c9a49a4d68d0733acdcad80ab0e72afa025a777fdc2966ceccf9d51765",
    "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.7.4.post1.tar.gz"
   },
   {
    "path": "pyproject.toml"
   },
   {
    "path": "setup.py"
   },
   {
    "sha256": "f03485c9a49a4d68d0733acdcad80ab0e72afa025a777fdc2966ceccf9d51765",
    "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.7.4.post1.tar.gz"
   },
   {
    "path": "pyproject.toml"
   },
   {
    "path": "setup.py"
   }
  ]
 },
 "linux_aarch64_requirements": {
  "build": {
   "__set__": true,
   "elements": [
    "c_compiler_stub",
    "c_stdlib_stub",
    "cuda_compiler_stub",
    "cxx_compiler_stub",
    "ninja"
   ]
  },
  "host": {
   "__set__": true,
   "elements": [
    "cuda-cudart-dev",
    "cuda-version",
    "libcublas-dev",
    "libcurand-dev",
    "libcusolver-dev",
    "libcusparse-dev",
    "libtorch",
    "pip",
    "python",
    "pytorch",
    "setuptools"
   ]
  },
  "run": {
   "__set__": true,
   "elements": [
    "einops",
    "flash-attn",
    "python",
    "pytorch"
   ]
  },
  "test": {
   "__set__": true,
   "elements": [
    "pip"
   ]
  }
 },
 "meta_yaml": {
  "about": {
   "home": "https://github.com/Dao-AILab/flash-attention",
   "license": "BSD-3-Clause",
   "license_file": [
    "LICENSE",
    "LICENSE_CUTLASS.txt"
   ],
   "summary": "Flash Attention: Fast and Memory-Efficient Exact Attention"
  },
  "build": {
   "number": "0",
   "script": "PYTHON -m pip install . -vvv --no-deps --no-build-isolation",
   "script_env": [
    "MAX_JOBS=4",
    "TORCH_CUDA_ARCH_LIST=8.0;8.6;8.9;9.0+PTX"
   ]
  },
  "extra": {
   "feedstock-name": "flash-attn",
   "recipe-maintainers": [
    "carterbox",
    "weiji14"
   ]
  },
  "outputs": [
   {
    "files": {
     "include": [
      "lib/python*/site-packages/flash_attn/**",
      "lib/python*/site-packages/flash_attn-2.7.4.post1.dist-info/**",
      "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
     ]
    },
    "name": "flash-attn",
    "requirements": {
     "build": [
      "c_compiler_stub",
      "cxx_compiler_stub",
      "cuda_compiler_stub",
      "c_stdlib_stub"
     ],
     "host": [
      "cuda-version 12.6",
      "cuda-cudart-dev",
      "python",
      "libtorch",
      "pytorch",
      "pytorch =*=cuda*"
     ],
     "run": [
      "einops",
      "python",
      "pytorch =*=cuda*"
     ]
    },
    "test": {
     "commands": [
      "pip check"
     ],
     "imports": [
      "flash_attn"
     ],
     "requires": [
      "pip"
     ]
    }
   },
   {
    "files": {
     "include": [
      "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
     ]
    },
    "name": "flash-attn-fused-dense",
    "requirements": {
     "build": [
      "cxx_compiler_stub",
      "c_stdlib_stub",
      "cuda_compiler_stub"
     ],
     "host": [
      "cuda-version 12.6",
      "cuda-cudart-dev",
      "libcublas-dev",
      "python",
      "pytorch",
      "pytorch =*=cuda*"
     ],
     "run": [
      "python",
      "flash-attn"
     ]
    },
    "test": {
     "commands": [
      "pip check"
     ],
     "imports": [
      "flash_attn.ops.fused_dense"
     ],
     "requires": [
      "pip"
     ]
    }
   },
   {
    "files": {
     "include": [
      "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
     ]
    },
    "name": "flash-attn-layer-norm",
    "requirements": {
     "build": [
      "cxx_compiler_stub",
      "c_stdlib_stub",
      "cuda_compiler_stub"
     ],
     "host": [
      "cuda-version 12.6",
      "cuda-cudart-dev",
      "python",
      "pytorch",
      "pytorch =*=cuda*"
     ],
     "run": [
      "python",
      "flash-attn"
     ]
    },
    "test": {
     "commands": [
      "pip check"
     ],
     "imports": [
      "flash_attn.ops.layer_norm"
     ],
     "requires": [
      "pip"
     ]
    }
   },
   {
    "files": {
     "include": [
      "lib/python*/site-packages/flash_attn/**",
      "lib/python*/site-packages/flash_attn-2.7.4.post1.dist-info/**",
      "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
     ]
    },
    "name": "flash-attn",
    "requirements": {
     "build": [
      "c_compiler_stub",
      "cxx_compiler_stub",
      "cuda_compiler_stub",
      "c_stdlib_stub"
     ],
     "host": [
      "cuda-version 12.6",
      "cuda-cudart-dev",
      "python",
      "libtorch",
      "pytorch",
      "pytorch =*=cuda*"
     ],
     "run": [
      "einops",
      "python",
      "pytorch =*=cuda*"
     ]
    },
    "test": {
     "commands": [
      "pip check"
     ],
     "imports": [
      "flash_attn"
     ],
     "requires": [
      "pip"
     ]
    }
   },
   {
    "files": {
     "include": [
      "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
     ]
    },
    "name": "flash-attn-fused-dense",
    "requirements": {
     "build": [
      "cxx_compiler_stub",
      "c_stdlib_stub",
      "cuda_compiler_stub"
     ],
     "host": [
      "cuda-version 12.6",
      "cuda-cudart-dev",
      "libcublas-dev",
      "python",
      "pytorch",
      "pytorch =*=cuda*"
     ],
     "run": [
      "python",
      "flash-attn"
     ]
    },
    "test": {
     "commands": [
      "pip check"
     ],
     "imports": [
      "flash_attn.ops.fused_dense"
     ],
     "requires": [
      "pip"
     ]
    }
   },
   {
    "files": {
     "include": [
      "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
     ]
    },
    "name": "flash-attn-layer-norm",
    "requirements": {
     "build": [
      "cxx_compiler_stub",
      "c_stdlib_stub",
      "cuda_compiler_stub"
     ],
     "host": [
      "cuda-version 12.6",
      "cuda-cudart-dev",
      "python",
      "pytorch",
      "pytorch =*=cuda*"
     ],
     "run": [
      "python",
      "flash-attn"
     ]
    },
    "test": {
     "commands": [
      "pip check"
     ],
     "imports": [
      "flash_attn.ops.layer_norm"
     ],
     "requires": [
      "pip"
     ]
    }
   },
   {
    "files": {
     "include": [
      "lib/python*/site-packages/flash_attn/**",
      "lib/python*/site-packages/flash_attn-2.7.4.post1.dist-info/**",
      "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
     ]
    },
    "name": "flash-attn",
    "requirements": {
     "build": [
      "c_compiler_stub",
      "cxx_compiler_stub",
      "cuda_compiler_stub",
      "c_stdlib_stub"
     ],
     "host": [
      "cuda-version 12.6",
      "cuda-cudart-dev",
      "python",
      "libtorch",
      "pytorch",
      "pytorch =*=cuda*"
     ],
     "run": [
      "einops",
      "python",
      "pytorch =*=cuda*"
     ]
    },
    "test": {
     "commands": [
      "pip check"
     ],
     "imports": [
      "flash_attn"
     ],
     "requires": [
      "pip"
     ]
    }
   },
   {
    "files": {
     "include": [
      "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
     ]
    },
    "name": "flash-attn-fused-dense",
    "requirements": {
     "build": [
      "cxx_compiler_stub",
      "c_stdlib_stub",
      "cuda_compiler_stub"
     ],
     "host": [
      "cuda-version 12.6",
      "cuda-cudart-dev",
      "libcublas-dev",
      "python",
      "pytorch",
      "pytorch =*=cuda*"
     ],
     "run": [
      "python",
      "flash-attn"
     ]
    },
    "test": {
     "commands": [
      "pip check"
     ],
     "imports": [
      "flash_attn.ops.fused_dense"
     ],
     "requires": [
      "pip"
     ]
    }
   },
   {
    "files": {
     "include": [
      "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
     ]
    },
    "name": "flash-attn-layer-norm",
    "requirements": {
     "build": [
      "cxx_compiler_stub",
      "c_stdlib_stub",
      "cuda_compiler_stub"
     ],
     "host": [
      "cuda-version 12.6",
      "cuda-cudart-dev",
      "python",
      "pytorch",
      "pytorch =*=cuda*"
     ],
     "run": [
      "python",
      "flash-attn"
     ]
    },
    "test": {
     "commands": [
      "pip check"
     ],
     "imports": [
      "flash_attn.ops.layer_norm"
     ],
     "requires": [
      "pip"
     ]
    }
   },
   {
    "files": {
     "include": [
      "lib/python*/site-packages/flash_attn/**",
      "lib/python*/site-packages/flash_attn-2.7.4.post1.dist-info/**",
      "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
     ]
    },
    "name": "flash-attn",
    "requirements": {
     "build": [
      "c_compiler_stub",
      "cxx_compiler_stub",
      "cuda_compiler_stub",
      "c_stdlib_stub"
     ],
     "host": [
      "cuda-version 12.6",
      "cuda-cudart-dev",
      "python",
      "libtorch",
      "pytorch",
      "pytorch =*=cuda*"
     ],
     "run": [
      "einops",
      "python",
      "pytorch =*=cuda*"
     ]
    },
    "test": {
     "commands": [
      "pip check"
     ],
     "imports": [
      "flash_attn"
     ],
     "requires": [
      "pip"
     ]
    }
   },
   {
    "files": {
     "include": [
      "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
     ]
    },
    "name": "flash-attn-fused-dense",
    "requirements": {
     "build": [
      "cxx_compiler_stub",
      "c_stdlib_stub",
      "cuda_compiler_stub"
     ],
     "host": [
      "cuda-version 12.6",
      "cuda-cudart-dev",
      "libcublas-dev",
      "python",
      "pytorch",
      "pytorch =*=cuda*"
     ],
     "run": [
      "python",
      "flash-attn"
     ]
    },
    "test": {
     "commands": [
      "pip check"
     ],
     "imports": [
      "flash_attn.ops.fused_dense"
     ],
     "requires": [
      "pip"
     ]
    }
   },
   {
    "files": {
     "include": [
      "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
     ]
    },
    "name": "flash-attn-layer-norm",
    "requirements": {
     "build": [
      "cxx_compiler_stub",
      "c_stdlib_stub",
      "cuda_compiler_stub"
     ],
     "host": [
      "cuda-version 12.6",
      "cuda-cudart-dev",
      "python",
      "pytorch",
      "pytorch =*=cuda*"
     ],
     "run": [
      "python",
      "flash-attn"
     ]
    },
    "test": {
     "commands": [
      "pip check"
     ],
     "imports": [
      "flash_attn.ops.layer_norm"
     ],
     "requires": [
      "pip"
     ]
    }
   },
   {
    "files": {
     "include": [
      "lib/python*/site-packages/flash_attn/**",
      "lib/python*/site-packages/flash_attn-2.7.4.post1.dist-info/**",
      "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
     ]
    },
    "name": "flash-attn",
    "requirements": {
     "build": [
      "c_compiler_stub",
      "cxx_compiler_stub",
      "cuda_compiler_stub",
      "c_stdlib_stub"
     ],
     "host": [
      "cuda-version 12.6",
      "cuda-cudart-dev",
      "python",
      "libtorch",
      "pytorch",
      "pytorch =*=cuda*"
     ],
     "run": [
      "einops",
      "python",
      "pytorch =*=cuda*"
     ]
    },
    "test": {
     "commands": [
      "pip check"
     ],
     "imports": [
      "flash_attn"
     ],
     "requires": [
      "pip"
     ]
    }
   },
   {
    "files": {
     "include": [
      "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
     ]
    },
    "name": "flash-attn-fused-dense",
    "requirements": {
     "build": [
      "cxx_compiler_stub",
      "c_stdlib_stub",
      "cuda_compiler_stub"
     ],
     "host": [
      "cuda-version 12.6",
      "cuda-cudart-dev",
      "libcublas-dev",
      "python",
      "pytorch",
      "pytorch =*=cuda*"
     ],
     "run": [
      "python",
      "flash-attn"
     ]
    },
    "test": {
     "commands": [
      "pip check"
     ],
     "imports": [
      "flash_attn.ops.fused_dense"
     ],
     "requires": [
      "pip"
     ]
    }
   },
   {
    "files": {
     "include": [
      "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
     ]
    },
    "name": "flash-attn-layer-norm",
    "requirements": {
     "build": [
      "cxx_compiler_stub",
      "c_stdlib_stub",
      "cuda_compiler_stub"
     ],
     "host": [
      "cuda-version 12.6",
      "cuda-cudart-dev",
      "python",
      "pytorch",
      "pytorch =*=cuda*"
     ],
     "run": [
      "python",
      "flash-attn"
     ]
    },
    "test": {
     "commands": [
      "pip check"
     ],
     "imports": [
      "flash_attn.ops.layer_norm"
     ],
     "requires": [
      "pip"
     ]
    }
   },
   {
    "files": {
     "include": [
      "lib/python*/site-packages/flash_attn/**",
      "lib/python*/site-packages/flash_attn-2.7.4.post1.dist-info/**",
      "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
     ]
    },
    "name": "flash-attn",
    "requirements": {
     "build": [
      "c_compiler_stub",
      "cxx_compiler_stub",
      "cuda_compiler_stub",
      "c_stdlib_stub"
     ],
     "host": [
      "cuda-version 12.6",
      "cuda-cudart-dev",
      "python",
      "libtorch",
      "pytorch",
      "pytorch =*=cuda*"
     ],
     "run": [
      "einops",
      "python",
      "pytorch =*=cuda*"
     ]
    },
    "test": {
     "commands": [
      "pip check"
     ],
     "imports": [
      "flash_attn"
     ],
     "requires": [
      "pip"
     ]
    }
   },
   {
    "files": {
     "include": [
      "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
     ]
    },
    "name": "flash-attn-fused-dense",
    "requirements": {
     "build": [
      "cxx_compiler_stub",
      "c_stdlib_stub",
      "cuda_compiler_stub"
     ],
     "host": [
      "cuda-version 12.6",
      "cuda-cudart-dev",
      "libcublas-dev",
      "python",
      "pytorch",
      "pytorch =*=cuda*"
     ],
     "run": [
      "python",
      "flash-attn"
     ]
    },
    "test": {
     "commands": [
      "pip check"
     ],
     "imports": [
      "flash_attn.ops.fused_dense"
     ],
     "requires": [
      "pip"
     ]
    }
   },
   {
    "files": {
     "include": [
      "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
     ]
    },
    "name": "flash-attn-layer-norm",
    "requirements": {
     "build": [
      "cxx_compiler_stub",
      "c_stdlib_stub",
      "cuda_compiler_stub"
     ],
     "host": [
      "cuda-version 12.6",
      "cuda-cudart-dev",
      "python",
      "pytorch",
      "pytorch =*=cuda*"
     ],
     "run": [
      "python",
      "flash-attn"
     ]
    },
    "test": {
     "commands": [
      "pip check"
     ],
     "imports": [
      "flash_attn.ops.layer_norm"
     ],
     "requires": [
      "pip"
     ]
    }
   },
   {
    "files": {
     "include": [
      "lib/python*/site-packages/flash_attn/**",
      "lib/python*/site-packages/flash_attn-2.7.4.post1.dist-info/**",
      "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
     ]
    },
    "name": "flash-attn",
    "requirements": {
     "build": [
      "c_compiler_stub",
      "cxx_compiler_stub",
      "cuda_compiler_stub",
      "c_stdlib_stub"
     ],
     "host": [
      "cuda-version 12.6",
      "cuda-cudart-dev",
      "python",
      "libtorch",
      "pytorch",
      "pytorch =*=cuda*"
     ],
     "run": [
      "einops",
      "python",
      "pytorch =*=cuda*"
     ]
    },
    "test": {
     "commands": [
      "pip check"
     ],
     "imports": [
      "flash_attn"
     ],
     "requires": [
      "pip"
     ]
    }
   },
   {
    "files": {
     "include": [
      "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
     ]
    },
    "name": "flash-attn-fused-dense",
    "requirements": {
     "build": [
      "cxx_compiler_stub",
      "c_stdlib_stub",
      "cuda_compiler_stub"
     ],
     "host": [
      "cuda-version 12.6",
      "cuda-cudart-dev",
      "libcublas-dev",
      "python",
      "pytorch",
      "pytorch =*=cuda*"
     ],
     "run": [
      "python",
      "flash-attn"
     ]
    },
    "test": {
     "commands": [
      "pip check"
     ],
     "imports": [
      "flash_attn.ops.fused_dense"
     ],
     "requires": [
      "pip"
     ]
    }
   },
   {
    "files": {
     "include": [
      "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
     ]
    },
    "name": "flash-attn-layer-norm",
    "requirements": {
     "build": [
      "cxx_compiler_stub",
      "c_stdlib_stub",
      "cuda_compiler_stub"
     ],
     "host": [
      "cuda-version 12.6",
      "cuda-cudart-dev",
      "python",
      "pytorch",
      "pytorch =*=cuda*"
     ],
     "run": [
      "python",
      "flash-attn"
     ]
    },
    "test": {
     "commands": [
      "pip check"
     ],
     "imports": [
      "flash_attn.ops.layer_norm"
     ],
     "requires": [
      "pip"
     ]
    }
   },
   {
    "files": {
     "include": [
      "lib/python*/site-packages/flash_attn/**",
      "lib/python*/site-packages/flash_attn-2.7.4.post1.dist-info/**",
      "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
     ]
    },
    "name": "flash-attn",
    "requirements": {
     "build": [
      "c_compiler_stub",
      "cxx_compiler_stub",
      "cuda_compiler_stub",
      "c_stdlib_stub"
     ],
     "host": [
      "cuda-version 12.6",
      "cuda-cudart-dev",
      "python",
      "libtorch",
      "pytorch",
      "pytorch =*=cuda*"
     ],
     "run": [
      "einops",
      "python",
      "pytorch =*=cuda*"
     ]
    },
    "test": {
     "commands": [
      "pip check"
     ],
     "imports": [
      "flash_attn"
     ],
     "requires": [
      "pip"
     ]
    }
   },
   {
    "files": {
     "include": [
      "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
     ]
    },
    "name": "flash-attn-fused-dense",
    "requirements": {
     "build": [
      "cxx_compiler_stub",
      "c_stdlib_stub",
      "cuda_compiler_stub"
     ],
     "host": [
      "cuda-version 12.6",
      "cuda-cudart-dev",
      "libcublas-dev",
      "python",
      "pytorch",
      "pytorch =*=cuda*"
     ],
     "run": [
      "python",
      "flash-attn"
     ]
    },
    "test": {
     "commands": [
      "pip check"
     ],
     "imports": [
      "flash_attn.ops.fused_dense"
     ],
     "requires": [
      "pip"
     ]
    }
   },
   {
    "files": {
     "include": [
      "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
     ]
    },
    "name": "flash-attn-layer-norm",
    "requirements": {
     "build": [
      "cxx_compiler_stub",
      "c_stdlib_stub",
      "cuda_compiler_stub"
     ],
     "host": [
      "cuda-version 12.6",
      "cuda-cudart-dev",
      "python",
      "pytorch",
      "pytorch =*=cuda*"
     ],
     "run": [
      "python",
      "flash-attn"
     ]
    },
    "test": {
     "commands": [
      "pip check"
     ],
     "imports": [
      "flash_attn.ops.layer_norm"
     ],
     "requires": [
      "pip"
     ]
    }
   },
   {
    "files": {
     "include": [
      "lib/python*/site-packages/flash_attn/**",
      "lib/python*/site-packages/flash_attn-2.7.4.post1.dist-info/**",
      "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
     ]
    },
    "name": "flash-attn",
    "requirements": {
     "build": [
      "c_compiler_stub",
      "cxx_compiler_stub",
      "cuda_compiler_stub",
      "c_stdlib_stub"
     ],
     "host": [
      "cuda-version 12.6",
      "cuda-cudart-dev",
      "python",
      "libtorch",
      "pytorch",
      "pytorch =*=cuda*"
     ],
     "run": [
      "einops",
      "python",
      "pytorch =*=cuda*"
     ]
    },
    "test": {
     "commands": [
      "pip check"
     ],
     "imports": [
      "flash_attn"
     ],
     "requires": [
      "pip"
     ]
    }
   },
   {
    "files": {
     "include": [
      "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
     ]
    },
    "name": "flash-attn-fused-dense",
    "requirements": {
     "build": [
      "cxx_compiler_stub",
      "c_stdlib_stub",
      "cuda_compiler_stub"
     ],
     "host": [
      "cuda-version 12.6",
      "cuda-cudart-dev",
      "libcublas-dev",
      "python",
      "pytorch",
      "pytorch =*=cuda*"
     ],
     "run": [
      "python",
      "flash-attn"
     ]
    },
    "test": {
     "commands": [
      "pip check"
     ],
     "imports": [
      "flash_attn.ops.fused_dense"
     ],
     "requires": [
      "pip"
     ]
    }
   },
   {
    "files": {
     "include": [
      "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
     ]
    },
    "name": "flash-attn-layer-norm",
    "requirements": {
     "build": [
      "cxx_compiler_stub",
      "c_stdlib_stub",
      "cuda_compiler_stub"
     ],
     "host": [
      "cuda-version 12.6",
      "cuda-cudart-dev",
      "python",
      "pytorch",
      "pytorch =*=cuda*"
     ],
     "run": [
      "python",
      "flash-attn"
     ]
    },
    "test": {
     "commands": [
      "pip check"
     ],
     "imports": [
      "flash_attn.ops.layer_norm"
     ],
     "requires": [
      "pip"
     ]
    }
   },
   {
    "files": {
     "include": [
      "lib/python*/site-packages/flash_attn/**",
      "lib/python*/site-packages/flash_attn-2.7.4.post1.dist-info/**",
      "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
     ]
    },
    "name": "flash-attn",
    "requirements": {
     "build": [
      "c_compiler_stub",
      "cxx_compiler_stub",
      "cuda_compiler_stub",
      "c_stdlib_stub"
     ],
     "host": [
      "cuda-version 12.6",
      "cuda-cudart-dev",
      "python",
      "libtorch",
      "pytorch",
      "pytorch =*=cuda*"
     ],
     "run": [
      "einops",
      "python",
      "pytorch =*=cuda*"
     ]
    },
    "test": {
     "commands": [
      "pip check"
     ],
     "imports": [
      "flash_attn"
     ],
     "requires": [
      "pip"
     ]
    }
   },
   {
    "files": {
     "include": [
      "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
     ]
    },
    "name": "flash-attn-fused-dense",
    "requirements": {
     "build": [
      "cxx_compiler_stub",
      "c_stdlib_stub",
      "cuda_compiler_stub"
     ],
     "host": [
      "cuda-version 12.6",
      "cuda-cudart-dev",
      "libcublas-dev",
      "python",
      "pytorch",
      "pytorch =*=cuda*"
     ],
     "run": [
      "python",
      "flash-attn"
     ]
    },
    "test": {
     "commands": [
      "pip check"
     ],
     "imports": [
      "flash_attn.ops.fused_dense"
     ],
     "requires": [
      "pip"
     ]
    }
   },
   {
    "files": {
     "include": [
      "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
     ]
    },
    "name": "flash-attn-layer-norm",
    "requirements": {
     "build": [
      "cxx_compiler_stub",
      "c_stdlib_stub",
      "cuda_compiler_stub"
     ],
     "host": [
      "cuda-version 12.6",
      "cuda-cudart-dev",
      "python",
      "pytorch",
      "pytorch =*=cuda*"
     ],
     "run": [
      "python",
      "flash-attn"
     ]
    },
    "test": {
     "commands": [
      "pip check"
     ],
     "imports": [
      "flash_attn.ops.layer_norm"
     ],
     "requires": [
      "pip"
     ]
    }
   }
  ],
  "package": {
   "name": "flash-attn-split",
   "version": "2.7.4"
  },
  "requirements": {
   "build": [
    "c_compiler_stub",
    "cxx_compiler_stub",
    "cuda_compiler_stub",
    "c_stdlib_stub",
    "ninja"
   ],
   "host": [
    "cuda-version 12.6",
    "cuda-cudart-dev",
    "libcublas-dev",
    "libcurand-dev",
    "libcusolver-dev",
    "libcusparse-dev",
    "libtorch",
    "pip",
    "python",
    "pytorch",
    "pytorch =*=cuda*",
    "setuptools"
   ]
  },
  "schema_version": 0,
  "source": [
   {
    "sha256": "f03485c9a49a4d68d0733acdcad80ab0e72afa025a777fdc2966ceccf9d51765",
    "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.7.4.post1.tar.gz"
   },
   {
    "path": "pyproject.toml"
   },
   {
    "path": "setup.py"
   },
   {
    "sha256": "f03485c9a49a4d68d0733acdcad80ab0e72afa025a777fdc2966ceccf9d51765",
    "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.7.4.post1.tar.gz"
   },
   {
    "path": "pyproject.toml"
   },
   {
    "path": "setup.py"
   },
   {
    "sha256": "f03485c9a49a4d68d0733acdcad80ab0e72afa025a777fdc2966ceccf9d51765",
    "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.7.4.post1.tar.gz"
   },
   {
    "path": "pyproject.toml"
   },
   {
    "path": "setup.py"
   },
   {
    "sha256": "f03485c9a49a4d68d0733acdcad80ab0e72afa025a777fdc2966ceccf9d51765",
    "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.7.4.post1.tar.gz"
   },
   {
    "path": "pyproject.toml"
   },
   {
    "path": "setup.py"
   },
   {
    "sha256": "f03485c9a49a4d68d0733acdcad80ab0e72afa025a777fdc2966ceccf9d51765",
    "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.7.4.post1.tar.gz"
   },
   {
    "path": "pyproject.toml"
   },
   {
    "path": "setup.py"
   },
   {
    "sha256": "f03485c9a49a4d68d0733acdcad80ab0e72afa025a777fdc2966ceccf9d51765",
    "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.7.4.post1.tar.gz"
   },
   {
    "path": "pyproject.toml"
   },
   {
    "path": "setup.py"
   },
   {
    "sha256": "f03485c9a49a4d68d0733acdcad80ab0e72afa025a777fdc2966ceccf9d51765",
    "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.7.4.post1.tar.gz"
   },
   {
    "path": "pyproject.toml"
   },
   {
    "path": "setup.py"
   },
   {
    "sha256": "f03485c9a49a4d68d0733acdcad80ab0e72afa025a777fdc2966ceccf9d51765",
    "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.7.4.post1.tar.gz"
   },
   {
    "path": "pyproject.toml"
   },
   {
    "path": "setup.py"
   },
   {
    "sha256": "f03485c9a49a4d68d0733acdcad80ab0e72afa025a777fdc2966ceccf9d51765",
    "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.7.4.post1.tar.gz"
   },
   {
    "path": "pyproject.toml"
   },
   {
    "path": "setup.py"
   },
   {
    "sha256": "f03485c9a49a4d68d0733acdcad80ab0e72afa025a777fdc2966ceccf9d51765",
    "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.7.4.post1.tar.gz"
   },
   {
    "path": "pyproject.toml"
   },
   {
    "path": "setup.py"
   }
  ]
 },
 "name": "flash-attn-split",
 "outputs_names": {
  "__set__": true,
  "elements": [
   "flash-attn",
   "flash-attn-fused-dense",
   "flash-attn-layer-norm"
  ]
 },
 "parsing_error": false,
 "platforms": [
  "linux_64",
  "linux_aarch64"
 ],
 "pr_info": {
  "__lazy_json__": "pr_info/flash-attn.json"
 },
 "raw_meta_yaml": "{% set version = \"2.7.4.post1\" %}\n# TODO: Add archs 10.0 and 12.0 with CTK 12.8 release\n\npackage:\n  name: flash-attn-split\n  # Strip \".postX\" from version number because these just indicate wheel rebuilds and\n  # are not relevant to conda builds\n  version: {{ version.split('.')[:3] | join('.') }}\n\nsource:\n  # Use PYPI sdist instead of GitHub because they include a specific revended CUTLASS\n  - url: https://pypi.org/packages/source/f/flash-attn/flash_attn-{{ version }}.tar.gz\n    sha256: f03485c9a49a4d68d0733acdcad80ab0e72afa025a777fdc2966ceccf9d51765\n  # Overwrite with a simpler build script that doesn't try to revend pre-compiled binaries\n  - path: pyproject.toml\n  - path: setup.py\n\nbuild:\n  number: 0\n  script: {{ PYTHON }} -m pip install . -vvv --no-deps --no-build-isolation\n  script_env:\n    # Limit MAX_JOBS in order to prevent runners from crashing\n    - MAX_JOBS=4\n    - TORCH_CUDA_ARCH_LIST=8.0;8.6;8.9;9.0+PTX\n  # Satisfy linter by not having duplicate skip keys\n  # pytorch in conda-forge does not support CUDA 11.8 as of v2.5\n  skip: true  # [cuda_compiler_version in (\"None\", \"11.8\") or (not linux)]\n  # debugging skips below\n  # skip: true  # [py!=313]\n\nrequirements:\n  build:\n    - {{ compiler('c') }}\n    - {{ compiler('cxx') }}\n    - {{ compiler('cuda') }}\n    - {{ stdlib('c') }}\n    - ninja\n    - pytorch           # [build_platform != target_platform]\n    - pytorch =*=cuda*  # [build_platform != target_platform]\n  host:\n    - cuda-version {{ cuda_compiler_version }}  # same cuda for host and build\n    - cuda-cudart-dev  # [(cuda_compiler_version or \"\").startswith(\"12\")]\n    - libcublas-dev    # [(cuda_compiler_version or \"\").startswith(\"12\")]\n    - libcurand-dev    # [(cuda_compiler_version or \"\").startswith(\"12\")]\n    - libcusolver-dev  # [(cuda_compiler_version or \"\").startswith(\"12\")]\n    - libcusparse-dev  # [(cuda_compiler_version or \"\").startswith(\"12\")]\n    - libtorch         # required until pytorch run_exports libtorch\n    - pip\n    - python\n    - pytorch\n    - pytorch =*=cuda*\n    - setuptools\n\noutputs:\n  - name: flash-attn\n    requirements:\n      build:\n        - {{ compiler('c') }}\n        - {{ compiler('cxx') }}\n        - {{ compiler('cuda') }}\n        - {{ stdlib('c') }}\n      host:\n        - cuda-version {{ cuda_compiler_version }}  # same cuda for host and build\n        - cuda-cudart-dev  # [(cuda_compiler_version or \"\").startswith(\"12\")]\n        - python\n        - libtorch         # required until pytorch run_exports libtorch\n        - pytorch\n        - pytorch =*=cuda*\n      run:\n        - einops\n        - python\n        - pytorch =*=cuda*\n\n    files:\n      include:\n        - 'lib/python*/site-packages/flash_attn/**'\n        - 'lib/python*/site-packages/flash_attn-{{ version }}.dist-info/**'\n        - 'lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so'\n\n    test:\n      imports:\n        - flash_attn\n      commands:\n        - pip check\n      requires:\n        - pip\n\n  - name: flash-attn-fused-dense\n    requirements:\n      build:\n        - {{ compiler('cxx') }} # needed for DSO checker\n        - {{ stdlib('c') }}     # needed for DSO checker\n        - {{ compiler('cuda') }}\n      host:\n        - cuda-version {{ cuda_compiler_version }}  # same cuda for host and build\n        - cuda-cudart-dev  # [(cuda_compiler_version or \"\").startswith(\"12\")]\n        - libcublas-dev    # [(cuda_compiler_version or \"\").startswith(\"12\")]\n        - python\n        - pytorch\n        - pytorch =*=cuda*\n      run:\n        - python\n        - {{ pin_subpackage('flash-attn', exact=True) }}\n\n    files:\n      include:\n        - 'lib/python*/site-packages/fused_dense_lib.cpython-*.so'\n\n    test:\n      imports:\n        - flash_attn.ops.fused_dense\n      commands:\n        - pip check\n      requires:\n        - pip\n\n  - name: flash-attn-layer-norm\n    requirements:\n      build:\n        - {{ compiler('cxx') }} # needed for DSO checker\n        - {{ stdlib('c') }}     # needed for DSO checker\n        - {{ compiler('cuda') }}\n      host:\n        - cuda-version {{ cuda_compiler_version }}  # same cuda for host and build\n        - cuda-cudart-dev  # [(cuda_compiler_version or \"\").startswith(\"12\")]\n        - python\n        - pytorch\n        - pytorch =*=cuda*\n      run:\n        - python\n        - {{ pin_subpackage('flash-attn', exact=True) }}\n\n    files:\n      include:\n        - 'lib/python*/site-packages/dropout_layer_norm.cpython-*.so'\n\n    test:\n      imports:\n        - flash_attn.ops.layer_norm\n      commands:\n        - pip check\n      requires:\n        - pip\n\nabout:\n  home: https://github.com/Dao-AILab/flash-attention\n  summary: 'Flash Attention: Fast and Memory-Efficient Exact Attention'\n  license: BSD-3-Clause\n  license_file:\n    - LICENSE\n    - LICENSE_CUTLASS.txt\n\nextra:\n  feedstock-name: flash-attn\n  recipe-maintainers:\n    - carterbox\n    - weiji14\n",
 "req": {
  "__set__": true,
  "elements": [
   "c_compiler_stub",
   "c_stdlib_stub",
   "cuda-cudart-dev",
   "cuda-version",
   "cuda_compiler_stub",
   "cxx_compiler_stub",
   "einops",
   "flash-attn",
   "libcublas-dev",
   "libcurand-dev",
   "libcusolver-dev",
   "libcusparse-dev",
   "libtorch",
   "ninja",
   "pip",
   "python",
   "pytorch",
   "setuptools"
  ]
 },
 "requirements": {
  "build": {
   "__set__": true,
   "elements": [
    "c_compiler_stub",
    "c_stdlib_stub",
    "cuda_compiler_stub",
    "cxx_compiler_stub",
    "ninja"
   ]
  },
  "host": {
   "__set__": true,
   "elements": [
    "c_compiler_stub",
    "c_stdlib_stub",
    "cuda-cudart-dev",
    "cuda-version",
    "cuda_compiler_stub",
    "cxx_compiler_stub",
    "libcublas-dev",
    "libcurand-dev",
    "libcusolver-dev",
    "libcusparse-dev",
    "libtorch",
    "pip",
    "python",
    "pytorch",
    "setuptools"
   ]
  },
  "run": {
   "__set__": true,
   "elements": [
    "c_compiler_stub",
    "c_stdlib_stub",
    "cuda_compiler_stub",
    "cxx_compiler_stub",
    "einops",
    "flash-attn",
    "python",
    "pytorch"
   ]
  },
  "test": {
   "__set__": true,
   "elements": [
    "pip"
   ]
  }
 },
 "strong_exports": false,
 "total_requirements": {
  "build": {
   "__set__": true,
   "elements": [
    "c_compiler_stub",
    "c_stdlib_stub",
    "cuda_compiler_stub",
    "cxx_compiler_stub",
    "ninja"
   ]
  },
  "host": {
   "__set__": true,
   "elements": [
    "cuda-cudart-dev",
    "cuda-version 12.6",
    "libcublas-dev",
    "libcurand-dev",
    "libcusolver-dev",
    "libcusparse-dev",
    "libtorch",
    "pip",
    "python",
    "pytorch",
    "pytorch =*=cuda*",
    "setuptools"
   ]
  },
  "run": {
   "__set__": true,
   "elements": [
    "einops",
    "flash-attn",
    "python",
    "pytorch =*=cuda*"
   ]
  },
  "test": {
   "__set__": true,
   "elements": [
    "pip"
   ]
  }
 },
 "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.7.4.post1.tar.gz",
 "version": "2.7.4",
 "version_pr_info": {
  "__lazy_json__": "version_pr_info/flash-attn.json"
 }
}